{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0242bbb1-a8e2-4c56-8747-246720260745",
   "metadata": {},
   "source": [
    "### Laboratorio 7\n",
    "---\n",
    "- Diego Alberto Leiva\n",
    "- Maria Marta Ramirez\n",
    "- Gustavo Andres Gonzales\n",
    "- José Pablo Orellana\n",
    "\n",
    "---\n",
    "### Task 1 - Preguntas Teóricas\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bc6f7-aa1c-49af-a78a-082b5a10e0b5",
   "metadata": {},
   "source": [
    "**¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b19cb-31b8-4836-b059-fd806cd2efba",
   "metadata": {},
   "source": [
    "- El Temporal Difference Learning es una técnica de aprendizaje por refuerzo que combina ideas del aprendizaje supervisado y el aprendizaje por diferencias temporales. A diferencia de los métodos tradicionales de aprendizaje supervisado, donde un agente aprende de un conjunto de pares de entrada-salida proporcionados de antemano, en TD Learning, el agente aprende a partir de la experiencia, actualizando sus estimaciones basándose en la diferencia entre las predicciones consecutivas (Sutton, R. S., & Barto, A. G, 2018).\n",
    "\n",
    "- El \"error de diferencia temporal\" es la diferencia entre la recompensa prevista en un estado y la combinación de la recompensa actual más la recompensa futura prevista. Este error se utiliza para actualizar las estimaciones de valor de un agente para mejorar su política de toma de decisiones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534eca8-d8a9-4aac-b216-2d7833df7ac0",
   "metadata": {},
   "source": [
    "**En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones\n",
    "de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego\n",
    "simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee7731-56d5-4425-81fc-8802ec83323f",
   "metadata": {},
   "source": [
    "- En los juegos simultáneos, los jugadores toman decisiones al mismo tiempo sin conocer las acciones de sus oponentes. Esto crea una situación de incertidumbre estratégica. Un ejemplo clásico de un juego simultáneo del mundo real es el Dilema del Prisionero, donde dos sospechosos son interrogados en salas separadas y deben decidir si confesar o no sin saber lo que el otro decidirá (Gibbons R, 1992).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3c0f6-4961-4e28-927c-0d619782a624",
   "metadata": {},
   "source": [
    "**¿Qué distingue los juegos de suma cero de los juegos de suma cero y cómo afecta esta diferencia al\n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren\n",
    "en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9c22f-b9c7-4cc6-8589-d22d7367d7e3",
   "metadata": {},
   "source": [
    "- Los juegos de suma cero son aquellos en los que la ganancia de un jugador es exactamente igual a la pérdida del otro. En contraste, en los juegos de no suma cero, las ganancias y pérdidas de los jugadores no necesariamente se cancelan mutuamente. Un ejemplo de un juego de no suma cero es el mercado bursátil, donde las transacciones pueden beneficiar a ambas partes. La principal diferencia afecta la toma de decisiones de los jugadores, ya que en los juegos de no suma cero pueden existir incentivos para cooperar o buscar resultados mutuamente beneficiosos (Osborne, M. J., & Rubinstein, A. (1994)).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863819c-72c0-4f41-85f3-e3f447c883f8",
   "metadata": {},
   "source": [
    "**¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de\n",
    "Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse\n",
    "unilateralmente de la estrategia elegida**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3bfff-769b-46c3-a454-d397010ed989",
   "metadata": {},
   "source": [
    "- El concepto de Equilibrio de Nash se aplica a los juegos simultáneos como una solución en la que ningún jugador se beneficia cambiando su estrategia mientras los demás mantengan la suya. Representa una situación de estabilidad en la que las estrategias de todos los jugadores son óptimas dados los comportamientos de los demás (Nash, J. (1950).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c498d4-5a4b-4074-b096-a7ab0415127e",
   "metadata": {},
   "source": [
    "**Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma\n",
    "de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre\n",
    "exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la\n",
    "práctica?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257607e-3b62-4272-9282-53e925304f8c",
   "metadata": {},
   "source": [
    "- El TD Learning se aplica en la modelación y optimización de procesos de toma de decisiones en entornos dinámicos equilibrando la exploración de nuevas estrategias con la explotación de las ya conocidas. Uno de los desafíos de su implementación es encontrar el balance adecuado entre exploración y explotación para maximizar el aprendizaje a largo plazo sin quedarse atrapado en estrategias subóptimas (Sutton, R. S., & Barto, A. G., 2018).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0de04-8962-488c-8855-58db97420a98",
   "metadata": {},
   "source": [
    "### Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a3182-eaf5-495e-83d9-a3f0ee459df9",
   "metadata": {},
   "source": [
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n",
    "- Gibbons, R. (1992). A primer in game theory. Harvester Wheatsheaf.\n",
    "- Osborne, M. J., & Rubinstein, A. (1994). A course in game theory. MIT Press.\n",
    "- Nash, J. (1950). Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36(1), 48-49."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
